<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Meghna Suresh, Author at Replicant</title>
	<atom:link href="https://www.replicant.com/blog/author/meghna-suresh/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.replicant.com/blog/author/meghna-suresh/</link>
	<description></description>
	<lastBuildDate>Wed, 14 Jun 2023 04:47:06 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.3.1</generator>

<image>
	<url>https://www.replicant.com/wp-content/uploads/2022/10/cropped-Symbol_SVG-1-32x32.png</url>
	<title>Meghna Suresh, Author at Replicant</title>
	<link>https://www.replicant.com/blog/author/meghna-suresh/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Replicant Labs: How LLMs Supercharge Our Contact Center Automation Platform</title>
		<link>https://www.replicant.com/blog/replicant-labs-how-llms-supercharge-our-contact-center-automation-platform/</link>
		
		<dc:creator><![CDATA[Meghna Suresh]]></dc:creator>
		<pubDate>Wed, 14 Jun 2023 01:43:52 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://www.replicant.com/?p=5883</guid>

					<description><![CDATA[<p>The Replicant Labs series pulls back the curtain on the tech, tools, and people behind...</p>
<p>The post <a rel="nofollow" href="https://www.replicant.com/blog/replicant-labs-how-llms-supercharge-our-contact-center-automation-platform/">Replicant Labs: How LLMs Supercharge Our Contact Center Automation Platform</a> appeared first on <a rel="nofollow" href="https://www.replicant.com">Replicant</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p><i><span style="font-weight: 400;">The Replicant Labs series pulls back the curtain on the tech, tools, and people behind the Thinking Machine. From double-clicks into the latest technical breakthroughs like Large Language Models to first-hand stories from our subject matter experts, Replicant Labs provides a deeper look into the work and people that make our customers better every day. </span></i></p>
<p><b>Meghna Suresh, Head of Product, Replicant</b></p>
<p><span style="font-weight: 400;">Replicant’s </span><a href="https://www.replicant.com/blog/how-replicant-unlocks-the-future-of-contact-center-automation-with-chatgpt-and-llms/" target="_blank" rel="noopener"><span style="font-weight: 400;">Large Language Model layer</span></a><span style="font-weight: 400;"> delivers more resolutions, faster time-to-value and strengthened security for contact centers. </span></p>
<p><span style="font-weight: 400;">With the latest neural network advancements, our LLM layer ushers in the next evolution of natural, automated customer service. And callers don’t need to change a thing to reap the benefits. </span></p>
<p><span style="font-weight: 400;">In our first live flow with the LLM layer – an intake flow collecting car details during emergency roadside service calls – the Thinking Machine achieved a soaring 90% resolution rate right out of the gate. </span></p>
<p><span style="font-weight: 400;">But it isn’t magic. Under the hood, we’ve taken a calculated approach to leverage the immense potential of LLMs like ChatGPT, while safeguarding contact centers from their risks. </span></p>
<p><span style="font-weight: 400;">Replicant knows that enterprise-level challenges require enterprise-grade solutions. The new features made possible by our LLM layer extend the Thinking Machine&#8217;s ability to improve customer experiences while protecting businesses from vulnerabilities. </span></p>
<p><span style="font-weight: 400;">Here’s a deeper look at what’s new:</span></p>
<p>&nbsp;</p>
<p><span style="font-weight: 400;"><img decoding="async" fetchpriority="high" class="wp-image-5886 aligncenter" src="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-1-300x182.jpg" alt="The first flow Replicant went live in production using our new LLM layer – collecting the make/model/year of cars for auto insurance – resulted in a soaring 90% resolution rate right out of the gate. " width="501" height="304" srcset="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-1-300x182.jpg 300w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-1-768x465.jpg 768w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-1.jpg 800w" sizes="(max-width: 501px) 100vw, 501px" /></span></p>
<h2><span style="font-weight: 400;">Resolve More Customer Issues</span></h2>
<p><span style="font-weight: 400;">Contact centers rely on the Thinking Machine for its best-in-class resolution rates and intent recognition accuracy. With LLMs, it can now better navigate common obstacles in a customer service conversation. </span></p>
<p><b>1-Turn Problem Capture. </b><span style="font-weight: 400;">Replicant’s LLM layer captures multiple intents and entities at once, then drives action off of it. A Thinking Machine being used to schedule healthcare appointments, for example, can simply ask a caller “How can I help you today?” The customer can respond with a natural sentence like, “I’d like to schedule a follow-up for my three-year-old next week,” and the Thinking Machine can quickly detect multiple intents, like appointment type and patient type, driving faster resolutions.</span></p>
<p><span style="font-weight: 400;"><img decoding="async" class="wp-image-5885 aligncenter" src="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-3-300x182.jpg" alt="The first flow Replicant went live in production using our new LLM layer – collecting the make/model/year of cars for auto insurance – resulted in a soaring 90% resolution rate right out of the gate. " width="499" height="303" srcset="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-3-300x182.jpg 300w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-3-768x465.jpg 768w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-3.jpg 800w" sizes="(max-width: 499px) 100vw, 499px" /></span></p>
<p><b>Logical Reasoning. </b><span style="font-weight: 400;">LLMs improve the deductive abilities of the Thinking Machine without requiring explicit training to do so. In the above example, the Thinking Machine implicitly understands that “for my three-year-old” means the appointment type is pediatric, and that a follow-up appointment signifies an existing patient, which can be verified against a CRM with the caller’s phone number or other authentication steps. </span></p>
<p><b>Contextual Disambiguation</b><strong><i>.</i></strong><span style="font-weight: 400;"> Customers often say things that are somewhat ambiguous. Older NLU models struggled to determine what action to take during these instances. For example, a customer trying to decide on an appointment time might say, “Actually, Tuesday morning might not work if my first meeting runs long.” Our LLM layer enables the Thinking Machine to be fully aware of the nuance in a customer’s intent, and immediately follow up with more appointment options that might work better. </span></p>
<p><strong>Dynamic Conversation Repair</strong><span style="font-weight: 400;"><strong>.</strong> After the Thinking Machine takes an action like scheduling an appointment, customers may still change their mind later in the conversation. They may remember they need to add a service, update a credit card, or even schedule another appointment. LLMs ensure seamless repair in conversation design, allowing the Thinking Machine to update a request on the fly without asking the customer to start over. </span></p>
<p><b>First-party Database Matching.</b><span style="font-weight: 400;"> LLMs have a vast knowledge of the world, but not of your business. Replicant’s LLM layer augments LLMs with your databases to match customers’ natural prompts to the context of your business. For example, if a customer booking a hotel room says “I want to stay in the city,” the Thinking Machine can reason that they’d prefer the downtown location for their stay rather than the airport location. </span></p>
<p><b>Intelligent Reconnect. </b><span style="font-weight: 400;">Replicant’s LLM layer keeps structured records of each conversation turn, which allows the Thinking Machine to know in realtime what’s previously happened in a conversation and what needs to happen next. When calls get disconnected, the LLM dialog engine can follow an intelligent reconnect workflow to pick up right where the customer left off when they call back.</span></p>
<p><span style="font-weight: 400;"><img decoding="async" class=" wp-image-5884 aligncenter" src="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-2-300x182.jpg" alt="The first flow Replicant went live in production using our new LLM layer – collecting the make/model/year of cars for auto insurance – resulted in a soaring 90% resolution rate right out of the gate. " width="501" height="304" srcset="https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-2-300x182.jpg 300w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-2-768x465.jpg 768w, https://www.replicant.com/wp-content/uploads/2023/06/Replicant-Labs-LLMs-2.jpg 800w" sizes="(max-width: 501px) 100vw, 501px" /></span></p>
<p><b>Complex Flows. </b><span style="font-weight: 400;">Replicant’s LLM layer allows the Thinking Machine to handle higher levels of complexity. It can parse different SKUs, modifications to a request and changes to order quantities, allowing customers to make even more in-depth requests. With the LLM layer, contact centers can think beyond the confines of simple use cases like FAQs to resolve more call drivers and scale automation even further. </span></p>
<h2><span style="font-weight: 400;">Faster Time-to-Value </span></h2>
<p><span style="font-weight: 400;">Contact centers can deploy a Thinking Machine in weeks to navigate the unpredictable world of customer service even faster, while keeping project timelines within scope.</span></p>
<p><b>Few Shot Learning. </b><span style="font-weight: 400;">Our LLM layer allows us to significantly reduce the development time required to customize a Thinking Machine around specific businesses and use cases by accelerating key design stages. Now, a solution can be optimized and deployed with fewer training cycles and minimal updating and retraining phases to achieve the best possible resolution rate in just weeks.</span></p>
<h2><span style="font-weight: 400;">Safeguards and Control </span></h2>
<p><span style="font-weight: 400;">Replicant’s LLM layer combines the power of the technologies like ChatGPT with the experience and security of our enterprise-grade Contact Center Automation platform. </span></p>
<p><b>Dialog Policy Control</b><span style="font-weight: 400;">. LLMs don’t just work out of the box. On their own, they are susceptible to inaccurate, inappropriate or offensive responses that make them unfit to be connected directly with customers. Our dialog policy validates every piece of data we get back against a coded set of rules that govern what the LLM can say and do in our platform, preventing inappropriate or offensive responses.</span></p>
<p><b>Prompt Engineering.</b><span style="font-weight: 400;"> Replicant’s platform follows prescribed workflows and scripts, just as agents do. </span><span style="font-weight: 400;">Our team has experience designing 10M+ conversations over six years and has created best practices to prompt and rigorously evaluate LLMs. Our workflow abstracts away the complexity of LLMs, making prompt-based conversation building highly accessible and reliable. </span></p>
<p><b>Always Secure. </b><span style="font-weight: 400;">Our architecture safeguards customers’ sensitive data and protects against security vulnerabilities like prompt injection. This means, first and foremost, that customers’ PII is never passed on to third-party LLMs. In addition, our LLM layer is never susceptible to being “tricked” by user prompts that might otherwise lead an LLM to give an inappropriate response. Thinking Machine responses are always designed and pre-configured to be 100% predictable for each business and use case.   </span></p>
<p><span style="font-weight: 400;">AI holds the ability to bring a wealth of promise – and potential peril – to your contact center. </span><a href="https://www.replicant.com/contact-center-automation/request-a-demo/" target="_blank" rel="noopener"><span style="font-weight: 400;">Come learn</span></a><span style="font-weight: 400;"> why contact center leaders continue to choose Replicant as their trusted partner for Contact Center Automation.</span></p>
<div id="om-sygm4yjbdvwv7fdbgulz-holder"></div>
<p>The post <a rel="nofollow" href="https://www.replicant.com/blog/replicant-labs-how-llms-supercharge-our-contact-center-automation-platform/">Replicant Labs: How LLMs Supercharge Our Contact Center Automation Platform</a> appeared first on <a rel="nofollow" href="https://www.replicant.com">Replicant</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
